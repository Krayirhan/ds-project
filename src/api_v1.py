"""
api_v1.py â€” V1 API router.

Original API endpoints mounted under /v1 prefix.
Maintains backward compatibility.
"""

from __future__ import annotations

import asyncio
import os

from fastapi import APIRouter, HTTPException, Request
import pandas as pd

from .api_shared import (
    ServingState,
    DecideResponse,
    ErrorResponse,
    PredictProbaResponse,
    RecordsPayload,
    ReloadResponse,
    load_serving_state,
)
from .config import ExperimentConfig
from .metrics import INFERENCE_ERRORS, INFERENCE_ROWS
from .predict import predict_with_policy, validate_and_prepare_features
from .tracing import trace_inference, set_span_attribute

router_v1 = APIRouter(prefix="/v1", tags=["v1"])

# Late-bound reference to app.state (set during include_router)
_app_ref = None


def _set_app_ref(app):
    global _app_ref
    _app_ref = app


def _get_serving_state() -> ServingState:
    if _app_ref is not None:
        serving = getattr(_app_ref.state, "serving", None)
        if serving is not None:
            return serving
    serving = load_serving_state()
    if _app_ref is not None:
        _app_ref.state.serving = serving
    return serving


@router_v1.post(
    "/predict_proba",
    response_model=PredictProbaResponse,
    responses={400: {"model": ErrorResponse}, 422: {"model": ErrorResponse}},
)
def v1_predict_proba(payload: RecordsPayload) -> PredictProbaResponse:
    try:
        serving = _get_serving_state()
        max_rows = ExperimentConfig().api.max_payload_records
        if len(payload.records) > max_rows:
            raise ValueError(f"Payload too large. Max records={max_rows}")
        df = pd.DataFrame(payload.records)
        with trace_inference(
            "v1.predict_proba",
            n_rows=len(df),
            model_name=str(getattr(serving.policy, "selected_model", "")),
        ):
            X, schema_report = validate_and_prepare_features(
                df, serving.feature_spec, fail_on_missing=True
            )
            proba = serving.model.predict_proba(X)[:, 1]
            set_span_attribute("ml.result_count", int(len(proba)))
        INFERENCE_ROWS.labels(
            endpoint="v1.predict_proba",
            model=str(getattr(serving.policy, "selected_model", "")),
        ).inc(len(proba))
        return PredictProbaResponse(
            n=int(len(proba)),
            proba=[float(x) for x in proba],
            schema_report=schema_report,
        )
    except Exception as e:
        _serving = getattr(_app_ref.state if _app_ref else None, "serving", None)
        _model = str(getattr(getattr(_serving, "policy", None), "selected_model", "") or "")
        INFERENCE_ERRORS.labels(endpoint="v1.predict_proba", model=_model).inc()
        raise HTTPException(status_code=400, detail=str(e))


@router_v1.post(
    "/decide",
    response_model=DecideResponse,
    responses={400: {"model": ErrorResponse}, 422: {"model": ErrorResponse}},
)
def v1_decide(payload: RecordsPayload) -> DecideResponse:
    try:
        serving = _get_serving_state()
        max_rows = ExperimentConfig().api.max_payload_records
        if len(payload.records) > max_rows:
            raise ValueError(f"Payload too large. Max records={max_rows}")
        df = pd.DataFrame(payload.records)
        with trace_inference(
            "v1.decide",
            n_rows=len(df),
            model_name=str(serving.policy.selected_model_artifact),
        ):
            actions_df, pred_report = predict_with_policy(
                model=serving.model,
                policy=serving.policy,
                df_input=df,
                feature_spec_payload=serving.feature_spec,
                model_used=serving.policy.selected_model_artifact,
            )
            set_span_attribute("ml.result_count", int(len(actions_df)))
        INFERENCE_ROWS.labels(
            endpoint="v1.decide",
            model=str(serving.policy.selected_model_artifact or ""),
        ).inc(len(actions_df))
        return DecideResponse(
            n=int(len(actions_df)),
            results=actions_df.to_dict(orient="records"),
            report=pred_report,
        )
    except Exception as e:
        _serving = getattr(_app_ref.state if _app_ref else None, "serving", None)
        _model = str(getattr(getattr(_serving, "policy", None), "selected_model_artifact", "") or "")
        INFERENCE_ERRORS.labels(endpoint="v1.decide", model=_model).inc()
        raise HTTPException(status_code=400, detail=str(e))


@router_v1.post(
    "/reload", response_model=ReloadResponse, responses={403: {"model": ErrorResponse}, 500: {"model": ErrorResponse}}
)
async def v1_reload(request: Request) -> ReloadResponse:
    expected_admin = os.getenv("DS_ADMIN_KEY")
    if expected_admin and request.headers.get("x-admin-key") != expected_admin:
        raise HTTPException(status_code=403, detail="x-admin-key header gereklidir.")
    lock = getattr(_app_ref.state if _app_ref else None, "_reload_lock", None) or asyncio.Lock()
    async with lock:
        try:
            serving = load_serving_state()
            if _app_ref is not None:
                _app_ref.state.serving = serving
            return {
                "status": "ok",
                "message": "Serving state reloaded",
                "model": serving.policy.selected_model,
                "policy_path": str(serving.policy_path),
            }
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Reload failed: {e}")
